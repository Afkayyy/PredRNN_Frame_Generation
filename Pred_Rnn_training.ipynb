{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Conv3D, ConvLSTM2D, Input, TimeDistributed, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tfj\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-03T17:25:02.085148Z",
     "iopub.status.busy": "2024-12-03T17:25:02.084838Z",
     "iopub.status.idle": "2024-12-03T17:25:02.116118Z",
     "shell.execute_reply": "2024-12-03T17:25:02.115423Z",
     "shell.execute_reply.started": "2024-12-03T17:25:02.085120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "dataset_path = \"/kaggle/input/ucf101-action-recognition\"\n",
    "train_csv = os.path.join(dataset_path, \"train.csv\")\n",
    "test_csv=os.path.join(dataset_path,\"test.csv\")\n",
    "val_csv=os.path.join(dataset_path,\"val.csv\")\n",
    "selected_classes = ['PullUps', 'PushUps', 'SoccerJuggling', 'SoccerPenalty', 'VolleyballSpiking']\n",
    "#load into dataframes\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df=pd.read_csv(test_csv)\n",
    "val_df=pd.read_csv(val_csv)\n",
    "\n",
    "\n",
    "selected_actions_df = train_df[train_df['label'].isin(selected_classes)]\n",
    "total_videos = len(selected_actions_df)\n",
    "video_paths = selected_actions_df['clip_path'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "base_path = \"/kaggle/input/ucf101-action-recognition\"\n",
    "actions_dir = \"train/selected_actions\"\n",
    "\n",
    "# List to store frames for all selected actions\n",
    "all_video_frames = []\n",
    "\n",
    "# Loop through each video file in the selected actions directory\n",
    "for file_name in os.listdir(os.path.join(base_path, actions_dir)):\n",
    "    video_relative_path = os.path.join(actions_dir, file_name)\n",
    "    video_full_path = os.path.join(base_path, video_relative_path)\n",
    "\n",
    "    # Check if the video file exists\n",
    "    if not os.path.exists(video_full_path):\n",
    "        print(f\"Error: File not found - {video_full_path}\")\n",
    "        continue\n",
    "\n",
    "    # Load the video using OpenCV\n",
    "    video_capture = cv2.VideoCapture(video_full_path)\n",
    "\n",
    "    if video_capture.isOpened():\n",
    "        frames_sequence = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = video_capture.read()\n",
    "            if not ret:\n",
    "                break  # Exit the loop when the video ends\n",
    "\n",
    "            # Resize the frame to 64x64 and convert to grayscale\n",
    "            resized_frame = cv2.resize(frame, (64, 64))\n",
    "            grayscale_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
    "            frames_sequence.append(grayscale_frame)\n",
    "\n",
    "        # Add frames of the current video to the main list\n",
    "        all_video_frames.append(frames_sequence)\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Unable to open video - {video_full_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T17:28:12.096294Z",
     "iopub.status.busy": "2024-12-03T17:28:12.095960Z",
     "iopub.status.idle": "2024-12-03T17:28:12.104465Z",
     "shell.execute_reply": "2024-12-03T17:28:12.103641Z",
     "shell.execute_reply.started": "2024-12-03T17:28:12.096263Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (71, 10, 64, 64), Output shape: (71, 5, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "input_seq_len = 10\n",
    "output_seq_len = 5\n",
    "\n",
    "def generate_sequences(frame_data, input_len, output_len):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    # Iterate to create input-output pairs\n",
    "    for idx in range(len(frame_data) - input_len - output_len):\n",
    "        input_data = frame_data[idx:idx + input_len]\n",
    "        target_data = frame_data[idx + input_len:idx + input_len + output_len]\n",
    "        \n",
    "        inputs.append(input_data)\n",
    "        targets.append(target_data)\n",
    "    \n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "# Use frames from the first video in the list\n",
    "selected_video_frames = all_video_frames[0]\n",
    "X_data, Y_data = generate_sequences(selected_video_frames, input_seq_len, output_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T17:28:13.202012Z",
     "iopub.status.busy": "2024-12-03T17:28:13.201455Z",
     "iopub.status.idle": "2024-12-03T17:28:25.725644Z",
     "shell.execute_reply": "2024-12-03T17:28:25.724923Z",
     "shell.execute_reply.started": "2024-12-03T17:28:13.201980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define STM Cell (modified from PredRNN)\n",
    "class STMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(STMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            input_channels + hidden_channels * 2, \n",
    "            hidden_channels * 4, \n",
    "            kernel_size, \n",
    "            padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h, c, m):\n",
    "        combined = torch.cat([x, h, m], dim=1)  # Combine input, hidden state, and memory\n",
    "        conv_out = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_out, self.hidden_channels, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        m_next = m + c_next  # Update spatio-temporal memory\n",
    "        return h_next, c_next, m_next\n",
    "\n",
    "# Define PredRNN Model with STM Cells\n",
    "class PredRNN(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, num_layers, output_channels):\n",
    "        super(PredRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.cells = nn.ModuleList([\n",
    "            STMCell(\n",
    "                input_channels if i == 0 else hidden_channels, \n",
    "                hidden_channels, \n",
    "                kernel_size\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        self.conv_out = nn.Conv2d(hidden_channels, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, predict_steps=1):\n",
    "        batch_size, seq_len, _, height, width = x.size()\n",
    "        h, c, m = self.init_hidden(batch_size, height, width, x.device)\n",
    "\n",
    "        # Process input sequence\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :, :, :]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                h[i], c[i], m[i] = cell(x_t, h[i], c[i], m[i])\n",
    "                x_t = h[i]\n",
    "\n",
    "        # Generate future frames\n",
    "        outputs = []\n",
    "        x_t = self.conv_out(h[-1])  # First predicted frame based on last hidden state\n",
    "        outputs.append(x_t)\n",
    "\n",
    "        for _ in range(predict_steps - 1):\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                h[i], c[i], m[i] = cell(x_t, h[i], c[i], m[i])\n",
    "                x_t = h[i]\n",
    "            x_t = self.conv_out(h[-1])\n",
    "            outputs.append(x_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "    def init_hidden(self, batch_size, height, width, device):\n",
    "        h = [torch.zeros(batch_size, self.hidden_channels, height, width).to(device) for _ in range(self.num_layers)]\n",
    "        c = [torch.zeros(batch_size, self.hidden_channels, height, width).to(device) for _ in range(self.num_layers)]\n",
    "        m = [torch.zeros(batch_size, self.hidden_channels, height, width).to(device) for _ in range(self.num_layers)]\n",
    "        return h, c, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T17:28:25.727326Z",
     "iopub.status.busy": "2024-12-03T17:28:25.726896Z",
     "iopub.status.idle": "2024-12-03T17:28:26.145568Z",
     "shell.execute_reply": "2024-12-03T17:28:26.144627Z",
     "shell.execute_reply.started": "2024-12-03T17:28:25.727299Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (14351, 10, 64, 64, 1), Y shape: (14351, 10, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Example: Dividing video frames into input-output segments\n",
    "def split_video_frames(frame_list, seq_len=10, target_len=5):\n",
    "    \"\"\"\n",
    "    Splits a sequence of video frames into input-output pairs for model training.\n",
    "\n",
    "    :param frame_list: List containing all video frames (64x64 grayscale).\n",
    "    :param seq_len: Number of frames in each input sequence.\n",
    "    :param target_len: Number of frames in each output sequence.\n",
    "    :return: Tuple (X_data, Y_data) of prepared input and output sequences.\n",
    "    \"\"\"\n",
    "    X_data, Y_data = [], []\n",
    "    num_frames = len(frame_list)\n",
    "\n",
    "    # Generate input and target sequences\n",
    "    for index in range(num_frames - seq_len - target_len + 1):\n",
    "        # Extract input sequence\n",
    "        X_data.append(frame_list[index : index + seq_len])\n",
    "\n",
    "        # Extract corresponding target sequence\n",
    "        Y_data.append(frame_list[index + seq_len : index + seq_len + target_len])\n",
    "\n",
    "    # Convert lists to numpy arrays and add channel dimension for grayscale images\n",
    "    X_data = np.expand_dims(np.array(X_data), axis=-1)  # Shape: (samples, seq_len, 64, 64, 1)\n",
    "    Y_data = np.expand_dims(np.array(Y_data), axis=-1)  # Shape: (samples, target_len, 64, 64, 1)\n",
    "    return X_data, Y_data\n",
    "\n",
    "# Flatten all video frames into a single sequence\n",
    "flattened_frames = [frame for video_clips in all_video_frames for frame in video_clips]\n",
    "\n",
    "# Define input and target sequence lengths\n",
    "seq_len = 10\n",
    "target_len = 10  # Updated to 10 frames for the target sequence\n",
    "\n",
    "# Generate input-output sequences\n",
    "X_data, Y_data = split_video_frames(flattened_frames, seq_len=seq_len, target_len=target_len)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"X_data shape: {X_data.shape}, Y_data shape: {Y_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T17:28:26.146699Z",
     "iopub.status.busy": "2024-12-03T17:28:26.146447Z",
     "iopub.status.idle": "2024-12-03T19:06:15.969333Z",
     "shell.execute_reply": "2024-12-03T19:06:15.968660Z",
     "shell.execute_reply.started": "2024-12-03T17:28:26.146675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733246909.972445     897 service.cc:145] XLA service 0x7c96e800aae0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733246909.972519     897 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1733246909.972524     897 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/1435\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:32:01\u001b[0m 9s/step - loss: 13235.6953 - mae: 88.7106"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733246917.155943     897 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 196ms/step - loss: 10834.0117 - mae: 78.3530 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 2/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10902.2041 - mae: 78.7203 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 3/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10862.8564 - mae: 78.5528 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 4/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10890.0166 - mae: 78.4851 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 5/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10864.3398 - mae: 78.5073 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 6/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10893.4570 - mae: 78.5412 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 7/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10937.4834 - mae: 78.9699 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 8/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10800.3232 - mae: 78.3572 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 9/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10729.2666 - mae: 77.9416 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 10/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10799.9736 - mae: 78.3131 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 11/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10859.9873 - mae: 78.5423 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 12/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 204ms/step - loss: 10884.5234 - mae: 78.7021 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 13/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 204ms/step - loss: 10881.0820 - mae: 78.7251 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 14/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 205ms/step - loss: 10841.2236 - mae: 78.4493 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 15/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 204ms/step - loss: 10707.8838 - mae: 77.9435 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 16/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10820.9053 - mae: 78.4503 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 17/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 205ms/step - loss: 10859.3516 - mae: 78.5304 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 18/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 204ms/step - loss: 10901.4238 - mae: 78.7518 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 19/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 204ms/step - loss: 10830.2832 - mae: 78.4004 - val_loss: 11363.1406 - val_mae: 83.2217\n",
      "Epoch 20/20\n",
      "\u001b[1m1435/1435\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 204ms/step - loss: 10872.1572 - mae: 78.5338 - val_loss: 11363.1406 - val_mae: 83.2217\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",  # Mean Squared Error for regression-like predictions\n",
    "    metrics=[\"mae\"]  # Mean Absolute Error\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X, Y,\n",
    "    epochs=20,  # Adjust epochs as needed\n",
    "    batch_size=8,  # Adjust batch size based on GPU memory\n",
    "    validation_split=0.2  # Use 20% of data for validation\n",
    ")\n",
    "model.save(\"predrnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from \"C:\\Users\\chabd\\Desktop\\deep\\deep\\models\\predrnn_model.pth\n",
      "Train Results: MSE: 0.0076, SSIM: 0.6183\n",
      "Test Results: MSE: 0.0171, SSIM: 0.4278\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Assuming the function `create_predrnn` is already defined as shown previously\n",
    "\n",
    "# Example: Preparing the data\n",
    "# Assuming input_output_pairs_per_class has the input-output pairs as described\n",
    "\n",
    "# Prepare the training data from input-output pairs\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for selected_class in selected_classes:\n",
    "    inputs = input_output_pairs_per_class[selected_class]['inputs']\n",
    "    outputs = input_output_pairs_per_class[selected_class]['outputs']\n",
    "    \n",
    "    X_train.extend(inputs)\n",
    "    y_train.extend(outputs)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)  # Shape: (num_samples, input_sequence_length, 64, 64, 3)\n",
    "y_train = np.array(y_train)  # Shape: (num_samples, output_sequence_length, 64, 64, 3)\n",
    "\n",
    "# Split data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create the PredRNN model\n",
    "input_sequence_length = 10  # Number of frames in the input sequence\n",
    "output_sequence_length = 5  # Number of frames to predict\n",
    "input_shape = (64, 64, 3)  # Each frame is 64x64 RGB image\n",
    "\n",
    "model = create_predrnn(input_sequence_length, output_sequence_length, input_shape)\n",
    "\n",
    "print(\"Model loaded from\" + os.path(predrnn_model.pth))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model with the prepared data\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=8, \n",
    "                    epochs=10, \n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "# Calculate MSE (Mean Squared Error)\n",
    "mse = np.mean((y_val - val_predictions) ** 2)\n",
    "    \n",
    "# Calculate SSIM (Structural Similarity Index)\n",
    "ssim_scores = []\n",
    "for i in range(len(val_predictions)):\n",
    "    ssim_score = tf.image.ssim(y_val[i], val_predictions[i], max_val=1.0)\n",
    "    ssim_scores.append(ssim_score.numpy())\n",
    "    \n",
    "avg_ssim = np.mean(ssim_scores)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Average SSIM: {avg_ssim}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"predrnn_model.h5\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2807884,
     "sourceId": 4849320,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 176090,
     "modelInstanceId": 153620,
     "sourceId": 180290,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 179153,
     "modelInstanceId": 156714,
     "sourceId": 183868,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
